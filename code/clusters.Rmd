---
title: "clusters "
output: html_document
date: "2023-01-18"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# this code will use the stats::kmeans() algorithm to identify clusters appearing in 
# weekly crab spider counts.

```


```{r bytheway}

# what is the rank of insect occurrence?
  
  skimr::skim(bugs.tibl)


```


```{r intro}

library(ggplot2)
library(gridExtra)
library(dplyr)

setwd("/Users/rcphelps/code/thesis/journalArticle/arXiv-preprint/code")

# source("./code/k-means.R")


source.url <- c("https://raw.githubusercontent.com/cordphelps/ampelos/master/data/bugs.csv")
bugs.tibl <- as_tibble(read.csv(source.url, header=TRUE, row.names=NULL) )

# remove all bugs except clab spider
bugs.tibl <- bugs.tibl %>% 
  dplyr::select(-Agapostemon.sp....green..native.bee., -Bombus.californicus..bumble., -Braconid.wasp, -checkerspot.butterfly, -Diabrotica.undecimpunctata..Cucumber.Beetle., -Diptera..Agromyzidae..leafminer.., -Halictus.sp....3.part..native.bee., -Honey.Bee, -ladyBug, -Lygus.hesperus..western.tarnished.plant.bug., -Orius..pirate.bug., -Osmia.sp...native.bee., -other, -pencilBug, -pentamonidae...stinkBug., -Pyralidae..Snout.Moth., -spider.other) 



```


```{r jaccard}

# note: jaccard and kmeans() don't seem to stand alone. need a more cohesive
# analysis of populations
# https://www.zoology.ubc.ca/~krebs/downloads/krebs_chapter_12_2017.pdf
# especially section 12.4
#


# from ampelosOriginal.Rmd

### each of the two transects consists of 3 rows of 10 traps in each row. Is the total insect population relatively uniform among the 3 rows of a transect? Does this uniformity change over time? Compute the Jaccard Index for each week: the index *'is a statistic used for comparing the similarity and diversity of sample sets.'* 

# Note that *'... the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets.'* (https://en.wikipedia.org/wiki/Jaccard_index)

source("./code/jaccard-Similarity.R")

g.oak <- compareJaccardMultiWeekV4(data=bugs.df, ignoreBees=TRUE,
                                  t="oakMargin",
                                  transectText="SNH",
                                  daytime='pm')

g.control <- compareJaccardMultiWeekV4(data=bugs.df, ignoreBees=TRUE,
                                  t="control",
                                  transectText="control",
                                  daytime='pm')

print(g.oak)
print(g.control)

```


```{r jaccard-stuff}

# from jaccard-Similarity.R


compareJaccardMultiWeekV4 <- function(data, ignoreBees, t, transectText, daytime) {

  # develop the data for similarity graphs that compare the populations of adjacent rows
  #
  # the resulting dataframe has similarity variables (jaccard and SME) for each
  # week by transect 
  #
  # this function is intended to be called from ampelos.Rmd
  #

  library(dplyr)

  #data <- bugs.df
  #ignoreBees <- TRUE
  #t <- "control"

  weeks.vector <- getWeeks(data)                    # determine weeks in the dataset
  weeks.df <- dplyr::bind_cols(week = weeks.vector) # place weeks in a column

  data <- data %>%  dplyr::rename(
                        Lygus.hesperus = Lygus.hesperus..western.tarnished.plant.bug., 
                        cucumber.beetle = Diabrotica.undecimpunctata..Cucumber.Beetle.) 

  captionComment <- paste("transect: ", transectText, "daytime: ", daytime, "\n(bees included)", sep="")

  if (ignoreBees == TRUE) { 

    captionComment <- paste("transect: ", transectText, " daytime: ", daytime, "\n(bees ignored)", sep="")

    data <- data %>% dplyr::select( 
      -Agapostemon.sp....green..native.bee.,
      -Bombus.californicus..bumble.,
      -checkerspot.butterfly,
      -cucumber.beetle,
      -Halictus.sp....3.part..native.bee.,
      -Honey.Bee,
      -Osmia.sp...native.bee.)

  }

  data <- data %>% 
    dplyr::filter(transect == t) %>% 
    dplyr::filter(time == daytime) %>%
    dplyr::select(-position, -positionX, -transect, -julian, -time, -date) 


  
  ## testing : > bugRowsJaccardSimilarityV2(df=bugs.df, t="control", w=getWeeks(data)[1])

## A tibble: 3 x 20
#    row Diptera..Agromyzid… Braconid.wasp Halictus.sp....3.p… pencilBug Agapostemon.sp...…
#  <dbl>               <dbl>         <int>               <dbl>     <dbl>              <dbl>
#1     1                   1             0                   1         1                  0
#2     1                   1             0                   1         0                  1
#3     1                   1             0                   1         0                  0
# ... with 14 more variables: Osmia.sp...native.bee. <dbl>, Honey.Bee <dbl>,
#   Bombus.californicus..bumble. <dbl>, Thomisidae..crab.spider. <dbl>,
#   spider.other <dbl>, ladyBug <dbl>, Lygus.hesperus <dbl>,
#   pentamonidae...stinkBug. <int>, other <dbl>, checkerspot.butterfly <dbl>,
#   Pyralidae..Snout.Moth. <dbl>, cucumber.beetle <dbl>, Orius..pirate.bug. <int>,
#   week <dbl>


  obs <- list()
  j <- 0
  temp.df <- data %>% dplyr::group_by(row) 

  for(i in weeks.vector) {

    j <- j + 1

    temp2.df <- temp.df %>%
      dplyr::filter(week == i) %>%
      dplyr::summarise_all(funs(sum)) 

    temp3.df <- ade4Similarity(temp2.df) # 

#> ade4Similarity(data)
#  Var1 Var2 left right   jaccard       SME
#1    1    2    1     2 0.5477226 0.4803845
#2    1    3    1     3 0.4472136 0.3922323
#3    2    3    2     3 0.6741999 0.6201737

#> mean(data$jaccard)   [1] 0.5563787 > mean(data$SME)  [1] 0.4975968

    obs[[j]] <- data.frame(transect=t, week=i, jaccard=mean(temp3.df$jaccard), SME=mean(temp3.df$SME))

  }

  # make one df for plotting
  plot.df <- dplyr::bind_rows(obs)

  #grid <- plotSimilarity(plot.df, transectText, captionComment)
  gg <- plotSimilarity(df=plot.df, text=captionComment)

  return(gg)

}



plotSimilarity <- function(df, text) {


  library(ggplot2)


  g <- ggplot(df) + 
      geom_jitter(aes(x=week, y=1-SME, colour = "mediumvioletred", fill = "plum1"), width = 0.1, height = 0.1, 
        show.legend = TRUE, shape = 21, size=5) + 
      geom_jitter(aes(x=week, y=1-jaccard, colour = "mediumvioletred", fill = "purple1"), width = 0.1, height = 0.1, 
        show.legend = TRUE, shape = 21, size=5) + 

      scale_fill_identity(name = 'method', guide = 'legend', breaks = c('plum1'='plum1','purple1'='purple1'), 
        labels = c('SMC','Jaccard')) +
      guides(colour=FALSE) +
      theme(legend.position = "right", legend.direction = "vertical") +

      ylim(c(0, 1)) + 
      # scale_y_continuous(breaks = seq(min(0), max(1), by = 0.1)) +
      expand_limits(y=c(0,1)) + 
      scale_x_continuous(breaks=seq(22,40,2)) +

      labs(
          y="index", 
          x="week", 
          caption = paste("transect: row triad population similarity, Jaccard and SMC", 
            "\nweekly mean of row-to-row indicies\n", 
            text, sep="" ) ) +

      #theme(legend.position="none") +
      theme(legend.position = "bottom", legend.direction = "horizontal") +
      theme_bw() 
      #coord_fixed(ratio=5) # control the aspect ratio of the output
      # https://stackoverflow.com/questions/7056836/how-to-fix-the-aspect-ratio-in-ggplot

  
    return(g)

  }


  ade4Similarity <- function(data) {

    # checking the logic by hand
    #
    #      1 obs1   1   0   1
    #      2 obs2   0   0   0
    #      3 obs3   0   0   1
    #      4 obs4   1   0   1
    #      5 obs5   1   1   0
    #      6 obs6   1   1   1
    #      7 obs7   1   0   0
    #      8 obs8   1   0   0
    #      9 obs9   1   0   1
    #
    # the jaccard distance between observation 1 and observation 3 is:
    #
    # - shared = count the number of matches (where a 1 in the first member matches a 1 in the second) :  1
    # - total = count the number of 1 vs 0 and 0 vs 1 : 1
    # - similarity coefficient : divide shared by (total plus shared) : .5
    # - distance : square root of 1 minus the similarity coefficient : .707
    #
    #
    # the jaccard distance between observation 5 and observation 6 is:
    #
    # - shared = count the number of matches (where a 1 in the first member matches a 1 in the second) :  2
    # - total = count the number of 1 vs 0 and 0 vs 1 : 1
    # - similarity coefficient : divide shared by (total plus shared) : .666
    # - distance : square root of 1 minus the similarity coefficient :  .577

    # R> # remove non-numeric values (the row names)
    # R> test.df <- test.df %>% dplyr::select(-X)
    # R> ade4::dist.binary(test.df, method=1, diag=F, upper=F)  
    # 
#          1         2         3         4         5         6         7         8         9
#1 0.0000000                                                                                
#2 1.0000000 0.0000000                                                                      
#3 0.7071068 1.0000000 0.0000000                                                            
#4 0.0000000 1.0000000 0.7071068 0.0000000                                                  
#5 0.8164966 1.0000000 1.0000000 0.8164966 0.0000000                                        
#6 0.5773503 1.0000000 0.8164966 0.5773503 0.5773503 0.0000000                              
#7 0.7071068 1.0000000 1.0000000 0.7071068 0.7071068 0.8164966 0.0000000                    
#8 0.7071068 1.0000000 1.0000000 0.7071068 0.7071068 0.8164966 0.0000000 0.0000000          
#9 0.0000000 1.0000000 0.7071068 0.0000000 0.8164966 0.5773503 0.7071068 0.7071068 0.0000000

    # https://pbil.univ-lyon1.fr/ade4/ade4-html/dist.binary.html
    # 
#> data
#  sp1 sp2 sp3
#1   1   0   1
#2   0   0   0
#3   0   0   1
#4   1   0   1
#5   1   1   0
#6   1   1   1
#7   1   0   0
#8   1   0   0
#9   1   0   1

    # add row numbers to the df
    # https://stackoverflow.com/questions/23518605/add-an-index-numeric-id-column-to-large-data-frame
    data <- tibble::rowid_to_column(data, "rowID")

#> data
#  rowID sp1 sp2 sp3
#1     1   1   0   1
#2     2   0   0   0
#3     3   0   0   1
#4     4   1   0   1
#5     5   1   1   0
#6     6   1   1   1
#7     7   1   0   0
#8     8   1   0   0
#9     9   1   0   1

    # save the list
    name.list <- data.frame(data$rowID)

#> name.list
#  data.rowID
#1          1
#2          2
#3          3
#4          4
#5          5
#6          6
#7          7
#8          8
#9          9

    # values > 1 re-written as 1
    data[data > 0] <- 1

    #row.names(data) <- name.list$data.row   # NOTE: apparently deprecated

    key_pairs <- expand.grid(name.list$data.row, name.list$data.row)  # column names are Var1 and Var2

    key_pair <- key_pairs %>% mutate(pair = paste(Var1,"_",Var2,sep=""))  # column names are Var1 and Var2
                                                                          # plus pair-ID in the form "row1-row2"


        # for each combination, make a 2 row matrix 
        #for (i in 1:nrow(key_pairs)) {   # 9 rows of 2 variables expands to 81 pairs
          #print(i)
          #print(key_pairs$Var1[i])
          #print(paste( slice ( data, key_pairs$Var1[i]   )) )
        #}

#> key_pairs
#........
#79    7    9
#80    8    9
#81    9    9
#> key_pairs$Var1[81]
#[1] 9
#> key_pairs$Var1[80]
#[1] 8
#> key_pairs$Var2[80]
#[1] 9

    # remove duplicates and mirrored data
    # walk through rows deleting all rows that are compared to themselves

    results <-  tibble::rownames_to_column(key_pair, var = "rowname")  # added a column "rowname",
                                                                            # contents are : "rowA_rowB"

    results2 <- key_pair %>% tidyr::separate("pair", c("left", "right"), "_") # replace column 'rowname' with two new rows
                                                                                    # contains 'rowA' and 'rowB'
    results3 <- results2 %>% dplyr::filter(results2$left != results2$right)  # remove self comparisons
    results4 <- results3 %>% dplyr::filter(results3$left < results3$right)   # remove duplicates (mirror images)

#> nrow(results4)
#[1] 36
#> results4
#   Var1 Var2 left right
#1     1    2    1     2
#2     1    3    1     3
#3     2    3    2     3
#4     1    4    1     4
#5     2    4    2     4
#6     3    4    3     4
#7     1    5    1     5


    data.matrix <- as.matrix(dplyr::select(data, -rowID))

#> data.matrix
#  sp1 sp2 sp3
#1   1   0   1
#2   0   0   0
#3   0   0   1
#4   1   0   1
#5   1   1   0
#6   1   1   1
#7   1   0   0
#8   1   0   0
#9   1   0   1

    
    new_column_val1.vector <- vector()   # init an empty vector to contain a distance measurement (from dist.ade4() )
    new_column_val2.vector <- vector()

    for (i in 1:nrow(results4)) {   # 9 rows of 2 variables expands to 81 pairs; reduces to 36 unique pairs

          #print(paste(  " sliceA ", data.matrix[ as.integer(results4$left[i]), ]  , 
          #              " sliceB ", data.matrix[ as.integer(results4$right[i]), ] , sep="") )

          leftData.matrix <- as.matrix(data.matrix[ as.integer(results4$left[i]), ], nrow=1)   # get data by a row number
          rightData.matrix <- as.matrix(data.matrix[ as.integer(results4$right[i]), ], nrow=1)  # get data by a row number

          # ! matrix filled column by column, not row by row !
          # join.matrix <- matrix(c(leftData.matrix, rightData.matrix), 2, 3)

          # (so work with data frames....)

          temp.df <- t(bind_cols(data.frame(leftData.matrix, row.names=NULL), data.frame(rightData.matrix, row.names=NULL)))

#>temp.df
#   V1 V2 V3
# 1  1  0  1
# 2  0  0  0
#   V1 V2 V3
# 1  1  0  1
# 2  0  0  1
#   V1 V2 V3
# 1  0  0  0
# 2  0  0  1

          #dist.binary(temp.df, method=1, diag=F, upper=F)  # output: 0.7071068   class='dist'
          #dist.binary(temp.df, method=2, diag=F, upper=F)  # output: 0.5773503

          # Jaccard distance
          # object dist is coerced to a matrix with as.matrix()
          row1.row2.m1.matrix <- as.matrix(ade4::dist.binary(temp.df, method=1, diag=F, upper=F))  
          # Simple Matching Coefficient distance
          row1.row2.m2.matrix <- as.matrix(ade4::dist.binary(temp.df, method=2, diag=F, upper=F)) 

#> row1.row2.m1.matrix
#          1         2
#1 0.0000000 0.7071068
#2 0.7071068 0.0000000

          # get the distance values
          jaccard.Distance <- row1.row2.m1.matrix[1,2]
          SME.Distance <- row1.row2.m2.matrix[1,2]


          # add them to the vector
          if (length(new_column_val1.vector)>0) {
            new_column_val1.vector <- c(new_column_val1.vector, jaccard.Distance)
            new_column_val2.vector <- c(new_column_val2.vector, SME.Distance)
          } else {
            new_column_val1.vector <- jaccard.Distance
            new_column_val2.vector <- SME.Distance
          }

    }

    # new_column_val1.vector now contains distance data for each 'unique' row conbination
    # a new column (for each distance value) that can be bound to result4 with dplyr::bind_cols()

    # add the vector(s) to the result4.df as a new column(s)
    results5 <- dplyr::bind_cols(results4, data.frame(jaccard=new_column_val1.vector))
    results5 <- dplyr::bind_cols(results5, data.frame(SME=new_column_val2.vector))

    
    return(results5)



  }

  
```


```{r chisq-distribution}

  positions.group.1 <- c(1, 2, 3)
  positions.group.2 <- c(4, 5, 6, 7)
  positions.group.3 <- c(8, 9, 10)
  
  whichData <- data
	#whichData <- bugs.tibl
	# two trap collection time periods : 'am' and 'pm'
  whichTime <- time
	#whichTime <- 'pm'
	# two transects : 'control' and 'oakMargin' (= semi-natural habitat / SNH)
	whichTransect <- 'oakMargin'
  SNH.group.1.new.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::filter(position %in% {{pos}}) %>%
    dplyr::select(-positionX, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.) %>%
    dplyr::group_by(week) %>%
    dplyr::summarize(crabSpiders=sum(crabSpiders), .groups='drop'  ) %>%

    dplyr::mutate(transect=whichTransect)

  chisq.p1.gg <- chiSqStatistics(data=bugs.tibl, 
                                 pos=positions.group.1, 
                                 time="pm")
  chisq.p2.gg <- chiSqStatistics(data=bugs.tibl, 
                                 pos=positions.group.2, 
                                 time="pm")
  chisq.p3.gg <- chiSqStatistics(data=bugs.tibl, 
                                 pos=positions.group.3,
                                 time="pm")
  
  

```


```{r chi-sq}

chiSqStatistics <- function(data, pos, time) {

# The KS-Test and other tests such as Anderson Darling are used for continuous
# distributions. For discrete distributions, you can use the Chi-Square goodness 
# of fit test, which is based on comparing the #observed events vs. the number 
# of expected based on the expected number for your distribution.
# https://stats.stackexchange.com/questions/55148/goodness-of-fit-to-poisson-distribution


# good graphical explanation of test statistics at the bottom
# https://www.jmp.com/en_sg/statistics-knowledge-portal/chi-square-test/chi-square-goodness-of-fit-test.html#:~:text=What%20is%20the%20Chi%2Dsquare,representative%20of%20the%20full%20population.
# 
  positionsCaption <- paste("vineyard rows ", pos[[1]], " - ", last(pos), sep="")

  whichData <- data
	#whichData <- bugs.tibl
	# two trap collection time periods : 'am' and 'pm'
  whichTime <- time
	#whichTime <- 'pm'
	# two transects : 'control' and 'oakMargin' (= semi-natural habitat / SNH)
	whichTransect <- 'oakMargin'
  SNH.group.1.new.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::filter(position %in% {{pos}}) %>%
    dplyr::select(-positionX, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.) %>%
    dplyr::group_by(week) %>%
    dplyr::summarize(crabSpiders=sum(crabSpiders), .groups='drop'  ) %>%

    dplyr::mutate(transect=whichTransect)
  
  #
  # result :  //  week  / crabSpiders / transect  //
  #

  whichTransect <- 'control'
  control.group.1.new.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::filter(position %in% positions.group.1) %>%
    dplyr::select(-transect, -positionX, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.) %>%
    dplyr::group_by(week) %>%
    dplyr::summarize(crabSpiders=sum(crabSpiders), .groups='drop'  ) %>%

    dplyr::mutate(transect=whichTransect)
  
  control.vector <- control.group.1.new.tibl %>% pull(crabSpiders)
  snh.vector <- SNH.group.1.new.tibl  %>% pull(crabSpiders)
  
  #
  # result : // (a vector of spider counts)
  #
  
  stats <- chisq.test(x=control.vector, y=snh.vector)
  
  # http://ianmadd.github.io/pages/Confidence_Intervals_Part4.html
  # probability based on a quantile
  probabilityDensity <- pchisq(stats$statistic, df = stats$parameter)
  #      .9368 = pchisq(48.58, df = 35)
  degreesFreedom <- stats$parameter
  # quantile based on a probability. (value of the x axis)
  chi95pct <- qchisq(.95, df=stats$parameter)
  #      49.80 = qchisq(.95, df = 35)
  #
  # ref : https://www.jmp.com/en_sg/statistics-knowledge-portal/chi-square-test/chi-square-goodness-of-fit-test.html
  
  # Using the Poisson distribution with μ=0.75 we can compute p(i), the 
  # hypothesised probabilities associated with each class (='bin'). From these 
  # we can calculate the expected frequencies (under the null hypothesis):
  # 
  
  xLabel.txt <- paste("Chi-Square Test Statistic, ", degreesFreedom,
                      " degrees of freedom", sep="")
  # "null hypothesis : each transect records the same distribution of spider counts \n", "p-value : ", stats$p.value, 
  tsLabel <- paste("test statistic : ", round(stats$statistic, 3), sep="")
      

      # https://stackoverflow.com/questions/48908296/chi-square-density-graph-in-r
  gg <- ggplot() +
    
      stat_function(fun = dchisq, args = list(df = degreesFreedom)) +
      # https://stackoverflow.com/questions/48753007/using-stat-function-to-draw-partially-shaded-normal-curve-in-ggplot2                 
      stat_function(fun = dchisq, args = list(df = degreesFreedom),
                   xlim = c(0, chi95pct),
                  geom = "area", fill = "green", alpha = .4) +

    expand_limits(x = c(0,75), y = c(0, 0.06)) +
    
    geom_vline(xintercept = chi95pct, linetype = 2) +
    geom_vline(xintercept = stats$statistic) +
    geom_label(aes(label="95% probability density"), 
                  x = 15, y = 0.03, fill = "white") +
    geom_segment(aes(x = 15, y = 0.057, xend = stats$statistic, yend = 0.053),
                  arrow = arrow(length = unit(0.5, "cm"))) +
    geom_label(aes(label=tsLabel), 
                  x = 15, y = 0.057, fill = "white") +
  
    labs(x = xLabel.txt, y = "probability per chi-sq unit",
      caption = paste(positionsCaption, "\nChi-Square Goodness of Fit\n",
         "Crab spider counts in the SNH transect\n",
         "compared to counts in the control transect\n", 
         "sampling time = ", time, sep="")) + 
    
    theme_bw() + 
    theme(legend.position="none")
  
  if (FALSE) {
  ##########################################################################
  ##########################################################################
  ############## begin chsq experimentation 
  ##########################################################################
  ##########################################################################
  
  
  # build : 
  # //   bin  /  frequency-observed  / hypo-prob  / expected-count / freq-expected  //

  meanControl <- mean(control.group.1.new.tibl$crabSpiders)
  meanSNH <- mean(SNH.group.1.new.tibl$crabSpiders)
  maxControl <- max(control.group.1.new.tibl$crabSpiders)
  maxSNH <- max(SNH.group.1.new.tibl$crabSpiders)
  
  # For a test of independence, df = (Rows − 1)×(Cols − 1), where in this case, 
  # Rows corresponds to the number of categories in one variable, and Cols
  # correspond to the number of categories in the second variable.[2]
  # https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test
  uniqueBinsControl.count <- dplyr::n_distinct(control.group.1.new.tibl$crabSpiders)
  uniqueBinsControl.list <- unique(control.group.1.new.tibl$crabSpiders)
  uniqueBinsSNH.count <- dplyr::n_distinct(SNH.group.1.new.tibl$crabSpiders)
  uniqueBinsSNH.list <- unique(SNH.group.1.new.tibl$crabSpiders)
  
  degreesFreedom <- (uniqueBinsControl.count -1 ) * (uniqueBinsSNH.count - 1)
  mu <- mean(SNH.group.1.new.tibl$crabSpiders)
  samples <- nrow(SNH.group.1.new.tibl)
  
  muPoissonSNH.tbl <- tribble(~bin, ~actualCount)
  for (i in 1:length(uniqueBinsSNH.list)) {
    # remove outliers
    if (uniqueBinsSNH.list[[i]] < 10) {
      binClass <- uniqueBinsSNH.list[[i]]
      frequency <- sum(snh.vector==uniqueBinsSNH.list[[i]], na.rm = TRUE)
    
      muPoissonSNH.tbl <- muPoissonSNH.tbl %>% 
                            add_row(bin= binClass, actualCount= frequency)
    }
  }
  # now mu for the sample can be calculated
  mu <- muPoissonSNH.tbl %>% sum(muPoissonSNH.tbl$bin * muPoissonSNH.tbl$actualCount) / samples
          
  
  chiSNH.tbl <- tribble(~bin, ~frequencyObserved, ~hypoProbability, 
                        ~countExpected, ~frequencyExpectedNull)
  chisq.X.SNH.lst <- list()
  chisq.prob.SNH.lst <- list()
  
  for (i in 1:length(uniqueBinsSNH.list)) { 
    
    binClass <- uniqueBinsSNH.list[[i]]
    fObs <- sum(snh.vector==uniqueBinsSNH.list[[i]], na.rm = TRUE)

    hp <- (  exp(- mu) * (mu**(binClass)) ) / factorial(binClass)
    fExp <- hp * (binClass)
    countExp <- samples * hp
    
    chiSNH.tbl <- chiSNH.tbl %>% 
                    add_row(bin= binClass,
                            frequencyObserved = fObs,
                            hypoProbability = hp,
                            countExpected = countExp,
                            frequencyExpectedNull = fExp
                            )

        if (FALSE) {
          # calculate probability of each count  (='bin')
          # https://bookdown.org/yg484/rec_4_note/chi-square-goodness-of-fit-test.html
          samples <- 11
          mu <- (meanControl + meanSNH) / 2
          binClass <- 0
          p0 <- (  exp(- mu) * (mu**binClass) ) / factorial(binClass)
          binCLass <- 1
          p1 <- (  exp(- mu) * (mu**binClass) ) / factorial(binClass)
          # et cetera for each bin
        }
      
  }
  
  ##########################################################################
  ##########################################################################
  ############## end chsq experimentation 
  ##########################################################################
  ##########################################################################
  }

  return(gg)
}

```


```{r mann-whitney-u-test}


	# 11 *unique* weeks are expected to satisfy the design of kmPlotNew()
	weeks.list <- getWeeks(bugs.tibl)

  positions.group.1 <- c(1, 2, 3)
  positions.group.2 <- c(4, 5, 6, 7)
  positions.group.3 <- c(8, 9, 10)
  
  weeksCaption <- paste("weeks ", pos[[1]], " - ", last[[pos]], sep="")

  box.p1.gg <- MWUgraphics(data= bugs.tibl, pos=positions.group.1, time='pm')
  box.p2.gg <- MWUgraphics(data= bugs.tibl, pos=positions.group.2, time='pm')
  box.p3.gg <- MWUgraphics(data= bugs.tibl, pos=positions.group.3, time='pm')
  
  #
  #
```
  
  
``` {r graphics-mann-whitney}
  
MWUgraphics <- function(data, pos, time) {

  # mirror https://stat-methods.com/home/mann-whitney-u-r/
  # MWUgraphics <- function(control, treatment) {
  
  positionsCaption <- paste("trap postions ", pos[[1]], " - ", last(pos), sep="")
  
	whichData <- data
	# two trap collection time periods : 'am' and 'pm'
	whichTime <- time
	# two transects : 'control' and 'oakMargin' (= semi-natural habitat / SNH)
	whichTransect <- 'oakMargin'
  SNH.group.1.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::select(-positionX, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.) %>%
    dplyr::group_by(week, position) %>%
    dplyr::summarize(crabSpiders=sum(crabSpiders), .groups='drop'  ) %>%
    dplyr::filter(position %in% {{pos}}) %>%
    dplyr::mutate(transect=whichTransect)

  whichTransect <- 'control'
  control.group.1.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::select(-transect, -positionX, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.) %>%
    dplyr::group_by(week, position) %>%
    dplyr::summarize(crabSpiders=sum(crabSpiders), .groups='drop'  ) %>%
    dplyr::filter(position %in% {{pos}}) %>%
    dplyr::mutate(transect=whichTransect)
  
  # append one tibble to the other
  merged.tibl <- dplyr::bind_rows(control.group.1.tibl, SNH.group.1.tibl) 
  # the tibble columns are
  #  // week : position : crabSpiders : transect //
  #

  
  if (FALSE) {
    library("gmodels")
    library("car")
    library("DescTools")
    merged.tibl %>% 
      select(transect, crabSpiders) %>% 
      group_by(transect) %>% 
      summarise(n = n(), 
            mean = mean(crabSpiders, na.rm = TRUE), 
            sd = sd(crabSpiders, na.rm = TRUE),
            stderr = sd/sqrt(n),
            LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,
            UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,
            median = median(crabSpiders, na.rm = TRUE),
            min = min(crabSpiders, na.rm = TRUE), 
            max = max(crabSpiders, na.rm = TRUE),
            IQR = IQR(crabSpiders, na.rm = TRUE),
            LCLmed = MedianCI(crabSpiders, na.rm=TRUE)[2],
            UCLmed = MedianCI(crabSpiders, na.rm=TRUE)[3])
  }
  
  
  # Test each group for normality
  # The Shapiro–Wilk test statistic (Calc W) is a measure of how well 
  # the ordered and standardized sample quantiles fit the standard normal 
  # quantiles. The statistic will take a value between 0 and 1 with 1 
  # being a perfect match.
  # Note that the result of the Shapiro–Wilk test should not be taken as 
  # being 100% reliable (no hypothesis test should). There will be cases 
  # where the test will give a misleading result. Therefore the Shapiro–Wilk 
  # test should never be used on its own – it should always be interpreted 
  # along with the results of graphical and numerical tools. In many cases, 
  # it will be obvious from the graphical and numerical tools that the data 
  # are (or are not) normally distributed, so the Shapiro–Wilk test will be 
  # unnecessary. It can be useful, however, when the results of the graphical 
  # and numerical tools are inconclusive.
  # https://www.sciencedirect.com/topics/mathematics/wilk-test
  
  normal.tibl <- merged.tibl %>%
    group_by(transect) %>%
    summarise(`W Stat` = shapiro.test(crabSpiders)$statistic,
              p.value = shapiro.test(crabSpiders)$p.value)
  

  if (FALSE) {
    #  QQ plots by group
    library("qqplotr")
    ggplot(data = merged.tibl, aes(sample = crabSpiders, color = transect, fill = transect)) +
      stat_qq_band(alpha=0.5, conf=0.95, qtype=1, bandType = "boot") +
      stat_qq_line(identity=TRUE) +
      stat_qq_point(col="black") +
      facet_wrap(~ transect, scales = "free") +
      labs(x = "Theoretical Quantiles", y = "Sample Quantiles") + theme_bw()
  }

  # Mann-Whitney U test
  # ?wilcox.test Details : Otherwise, if both x and y are given and paired 
  # is FALSE, a Wilcoxon rank sum test (equivalent to the Mann-Whitney test: 
  # see the Note) is carried out.
  # Optionally (if argument conf.int is true), a nonparametric confidence 
  # interval and an estimator for the pseudomedian (one-sample case) or 
  # for the difference of the location parameters x-y is computed.
  m1 <- wilcox.test(crabSpiders ~ transect, data=merged.tibl, na.rm=TRUE, 
                    paired=FALSE, exact=FALSE, conf.int=TRUE)

 
  # Hodges Lehmann Estimator
  # 'The Hodges–Lehmann statistic also estimates the difference between two populations.' per
  # https://en.wikipedia.org/wiki/Hodges%E2%80%93Lehmann_estimator
  #m1$estimate
  
  pv <- format(m1$p.value,nsmall=3)
  
  wilcoxon.txt <- paste("Mann-Whitney U Test ",
    "(=Wilcoxon rank sum test with continuity correction)\n",
    "null hypothesis: the two populations have the same continuous distribution\n",
    "p-value : ", pv, sep="")
                        
  ggBox <- ggplot() +
    
    stat_boxplot(geom ="errorbar", width = 0.5) +
    geom_boxplot(control.group.1.tibl, mapping=aes(x = transect, y = crabSpiders), 
                            fill = 'red') + 
    geom_boxplot(SNH.group.1.tibl, mapping=aes(x = transect, y = crabSpiders), 
                            fill = 'green') + 
    stat_summary(fun.y=mean, geom="point", shape=10, size=3.5, color="black") + 
              
    expand_limits(y = c(0,15)) +
    
    scale_x_discrete(labels=c("control" = "control", "oakMargin" = "SNH")) +

    labs(title = "poisson cumulative distribution function", 
        subtitle = paste(positionsCaption, "\n", 
                          wilcoxon.txt,
                          "\nSNH transect (green), control transect (red)",
                          sep=""),
        caption = "Mann-Whitney U",
                    x = "transect", y = "crab spider counts") + 
    theme_bw() + 
    theme(legend.position="none")
  
  
  return(ggBox)

}


```


```{r make-dist}

	# 11 *unique* weeks are expected to satisfy the design of kmPlotNew()
	weeks.list <- getWeeks(bugs.tibl)

	whichData <- bugs.tibl
	# two transects : 'control' and 'oakMargin' (= semi-natural habitat / SNH)
	whichTransect <- 'oakMargin'
	# two trap collection time periods : 'am' and 'pm'
	whichTime <- 'pm'

  SNH.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::select(-transect, -position, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.)
  # what remains is // row / positionX / crabSpiders / week //

  whichTransect <- 'control'
  
  control.tibl <- whichData %>%
    dplyr::filter(transect==whichTransect)  %>%
    dplyr::filter(time==whichTime) %>%
    dplyr::select(-transect, -position, -date, -time, -julian) %>%
    dplyr::rename(crabSpiders = Thomisidae..crab.spider.) 
  
  ggList <- makeDistributionGraphics(tibbleSNH=SNH.tibl, 
                                     tibbleControl=control.tibl,
                                     time=whichTime)
  
```

```{r dist-graphics}

makeDistributionGraphics <- function(tibbleSNH, tibbleControl, time) {
  
  # ecdf() ?   empirical cumulative distribution function
  # https://bjlkeng.github.io/posts/the-empirical-distribution-function/
  
  # https://rpubs.com/mpfoley73/456645
  # the PDF (probability *density*) and CDF (cumulative distribution) combo graph is
  # extremely confusing. x axes are different (continuous and discrete) and
  # y axes are different (probability per unit and probability)
  # don't overlay them
  
library(ggplot2)
library(dplyr)
options(scipen = 999, digits = 2) # sig digits


eventsSNH <- 0:max(tibbleSNH$crabSpiders)
eventsControl <- 0:max(tibbleControl$crabSpiders)


#  Poisson Distribution
densitySNH <- dpois(x = eventsSNH, lambda = mean(tibbleSNH$crabSpiders))
probSNH <- ppois(q = eventsSNH, lambda = mean(tibbleSNH$crabSpiders), lower.tail = TRUE)
# lambda is a 'rate', the mean number of spiders captured in a time interval
# note : Examples that violate the Poisson assumptions
# https://en.wikipedia.org/wiki/Poisson_distribution
densityControl <- dpois(x = eventsControl, lambda = mean(tibbleControl$crabSpiders))
probControl <- ppois(q = eventsControl, lambda = mean(tibbleControl$crabSpiders), lower.tail = TRUE)

  # a probability mass function is a function that gives the probability that a 
  # discrete random variable is exactly equal to some value.
  # a PMF differs from a probability density function (PDF) in that the latter 
  # is associated with continuous rather than discrete random variables. A PDF 
  # must be integrated over an interval to yield a probability
  # https://en.wikipedia.org/wiki/Probability_mass_function

  tiblSNH <- tibble(events, densitySNH, probSNH)
  tiblControl <- tibble(events, densityControl, probControl) 
  
  captionText <- paste("sampling time: ", time, sep="")

  ggPMF <- ggplot() +
  
    geom_col(data=tiblSNH, aes(x = factor(eventsSNH), y = densitySNH), 
             just = 0, width =.2, fill='green', color='black') +

    #geom_text(data=tiblSNH, aes(x = factor(eventsSNH), y = densitySNH + 0.01, 
                #                label = round(densitySNH, 2)), 
           # size = 3, vjust = 0, nudge_x = -.2 ) +
    
    geom_col(data=tiblControl, aes(x = factor(eventsControl), y = densityControl), 
             just = 1, width =.2, fill='red', color='black') +

    #geom_text(data=tiblControl, aes(x = factor(eventsControl), y = densityControl + 0.01, 
                            #    label = round(densityControl, 2)), 
           # size = 3, vjust = 0, nudge_x = .2 ) +
    
    expand_limits(y = c(0,1)) +

    labs(title = "poisson probability mass function", 
         subtitle = "SNH transect (green), control transect (red)",
         caption = captionText,    
         x = "crab spider counts", y = "probability mass") +
          
    theme_bw()

  ggCDF <- ggplot() +
  
    geom_point(data = tiblSNH, aes(x = factor(eventsSNH), y = probSNH), 
             fill = 'green', shape = 21, size=5) +
    
    geom_segment(data = tiblSNH, aes(x = as.numeric(eventsSNH) + 1, y = probSNH,
                    xend = as.numeric(eventsSNH) + 1.5, yend = probSNH), color = 'black') + 
    
    geom_point(data = tiblControl, aes(x = factor(eventsControl), y = probControl), 
             fill = 'red', shape = 21, size=5) +
    
    geom_segment(data = tiblControl, aes(x = as.numeric(eventsControl) + 1, y = probControl,
                    xend = as.numeric(eventsControl) + 1.5, yend = probControl), color = 'black') +
    
    expand_limits(y = c(.5,1)) +

    labs(title = "poisson cumulative distribution function", 
          subtitle = "SNH transect (green), control transect (red)",
          caption = captionText,
          x = "crab spider counts", y = "cumulative probability") +
  
    theme_bw()

  returnList <- list()
  returnList[[1]] <- ggPMF
  returnList[[2]] <- ggCDF
  
  hostDir <- '/Users/rcphelps/code/thesis/journalArticle/arXiv-preprint'
	where <- paste(hostDir, "/images", sep="")
	filenamePMF <- paste("dist-PMF-", "both", "-", time, "-.png", sep="")
	filenameCDF <- paste("dist-CDF-", "both", "-", time, "-.png", sep="")
	
	saveGraphics(graphics = ggPMF, name = filenamePMF, destinationPath = where) 
	saveGraphics(graphics = ggCDF, name = filenameCDF, destinationPath = where) 
  
  return(returnList)
}

```


```{r make-clusters}


	
	# 11 *unique* weeks are expected to satisfy the design of kmPlotNew()
	weeks.list <- getWeeks(bugs.tibl)


	
	whichData <- bugs.tibl
	# two transects : 'control' and 'oakMargin' (= semi-natural habitat / SNH)
	whichTransect <- 'oakMargin'
	# two trap collection time periods : 'am' and 'pm'
	whichTime <- 'pm'

	clusters.list <- list()
	clusters.tibl <- tibble::tibble('defaultColumn', .rows = 10)
	
		for (i in 1:length(weeks.list)) {

			  # currentWeek <- weeks.list[[i]]
        temp.tibl <- assignClusters(week=weeks.list[[i]], transect = whichTransect,
                                 time=whichTime, tibble=whichData, debug=FALSE)

        clusters.tibl <- dplyr::bind_cols(clusters.tibl, temp.tibl)
                                          
		  }
	
	# cleanup
	# set cluster integers to 'factors' so that ggplot
	# 'fill' will be determined by the integer value
	clusters.tibl <- clusters.tibl %>%
	                 dplyr::select(-'"defaultColumn"') %>%
	                 dplyr::mutate_all(as.factor) %>%
	                 dplyr::mutate(position=row_number()) 
	

  
	gg <- kmPlotNew(tibble = clusters.tibl, transectText = whichTransect, timeText = whichTime)
	
	hostDir <- '/Users/rcphelps/code/thesis/journalArticle/arXiv-preprint'
	where <- paste(hostDir, "/images", sep="")
	filename <- paste("clusters-", whichTransect, "-", whichTime, "-.png", sep="")
	
	saveGraphics(graphics = gg, name = filename, destinationPath = where) 
  
	  
	  
```


```{r general}

saveGraphics <- function(graphics, name, destinationPath) {
  
      # gg : ggplot output
      # name : filename with extension ('pdf')
      # dirPath : path to host directory relative to working directory
  
  
      # setwd("/Users/rcphelps/code/avo-soil-leaf-data/")
      # fileName <- paste("NK", "-monthly.pdf", sep="")
      # dirPath <- paste(wd, "/output", sep="")
  
      fullPath <- paste(destinationPath, "/", name, sep="")

      if (file.exists(fullPath)) { file.remove(fullPath) }

        # device not necessary if extension provided
      suppressMessages(ggsave(name, plot = graphics, device = NULL, 
                              path = destinationPath,
                              scale = 1, width = 3.8, height = 4, 
                              dpi = 300, limitsize = TRUE, units = "in") )
      
      if (file.exists(fullPath)) { 
        return(paste( "saved ", fullPath, sep=" "))
      }
      else {
        return(paste( "unable to save ", fullPath, sep=" "))
      }
      
}

```



```{r}


kmPlotNew <- function(tibble, transectText, timeText) {

	# build a plot that represents weekly clusters (1:3) by vineyard 
  # position (1:10)
  #
  # this result will be used to assess cluster position 'boundaries' 
  # that may appear as samples are taken from the vineyard edge 
  #
  # tibble : columns are "week23" "week24" ... "week32" and "position"
  #          rows are weekly cluster assignments as factors (1:3) 
  #          and row position as character (1:10) (there are 10 rows)
  #
  # weeks : a list, chr[1:10] "week23" "week24" ... "week34"
  # transectText :  text for informational subtitle content
  # timeText : text for informational subtitle content indicating 'am' or 'pm'
  
  if (transectText == 'oakMargin') {
    # adjust for source data ambiguity
    transectText <- 'SNH'
  }
  
  captionText <- paste("crab spider clusters\n", 
                       "transect: ", transectText, "\ndaytime: ", timeText, sep="")
  
  # get a list of the week labels ('weekXX')
  weeks <- colnames(tibble)
  # get a list of week IDs ('integers')
  weekID.list <- disectWeeks(w=weeks)
  
  
  clusterColors = c("3" = "blue", "2" = "green", "1" = "red", "0" = '#FFFFFF')

	gg <- ggplot(data = tibble) + 

  	      geom_point(aes(y = position, x = weeks[[1]], fill = week23),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[2]], fill = week24),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[3]], fill = week25),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[4]], fill = week26),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[5]], fill = week27),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[6]], fill = week28),  
  		         shape = 21, size=5) +
	  
  	      geom_point(aes(y = position, x = weeks[[7]], fill = week29),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[8]], fill = week30),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[9]], fill = week31),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[10]], fill = week32),  
  		         shape = 21, size=5) +
	  
	        geom_point(aes(y = position, x = weeks[[11]], fill = week34),  
  		         shape = 21, size=5) +

	        scale_fill_manual(values = clusterColors, 
	                    limits = names(clusterColors),
	                    labels = c("cluster 3", "cluster 2", "cluster 1")) +
	
    expand_limits(y=c(1,10)) +
    scale_y_continuous(breaks = seq(min(1), max(10), by = 1)) +
	  
	  scale_x_discrete(breaks=weeks, labels=weekID.list) +
    #expand_limits(x=c(22,34)) +    # enlarge the range of the axis
    #scale_x_discrete(breaks=seq(22,34,2)) +         # log, square-root, reverse....

    geom_hline(yintercept=5.5) +
    geom_hline(yintercept=8.5) +

    coord_fixed(ratio=1) + # control the aspect ratio

    labs(x="week number", y="trap position", caption = captionText ) +

  	theme_bw() +
    
	  # axis.title.x=element_text(angle=45, vjust=0.5),
	  # axis.text = element_text(colour = 1, size = 12),
	  #     legend.spacing.y = unit(0, "mm"), 
	  #     legend.title = element_blank(),
    #     legend.justification=c(1,0),
    #     legend.background = element_blank(),
    #     legend.box.background = element_rect(colour = "black")
    #     guide_legend(title="clusters") +
    #     guides(fill=guide_legend(title="clusters")) +
	  #     aspect.ratio = 1, 
	  
    theme(
          axis.text.x = element_text(angle = 45),
          panel.border = element_rect(colour = "black", fill=NA),
          legend.position="bottom",
          legend.title = element_blank()
          ) 
	
	return(gg)

}

```


```{r cluster-functions}
enoughData <- function(tibble, debug) {
  
  # check to see if there is enough data (non-zero spider counts)
  # to assign 3 clusters with kmeans() 
  
  # tibble input: for a specific week, spider counts for each positionX
  #
  #  week   positionX   crabSpiders
  
  
  
#             non-zero (of 11) weekly positional spider counts
#           (if count < 2,  clusters of 3 cannot be generated)
#
#  week  'oakMargin'+'am'  'oakMargin'+'pm' 'control'+'am' 'control'+'pm'
#         
#   23         5               10                8             11
#   24        11               11               11             11
#   25         9                9               10             10
#   26         9                9                7              9        
#   27         6                6                4              5
#   28         9               10                7              8
#   29         6                9                8              6
#   30         6                4                7              8
#   31         4                6                7              4
#   32         3                6                5              4
#   34         1                2                2              4

# code to generate the table above (this is the template for enoughData() )
if (FALSE) {
  zero.tibl <- bugs.tibl %>%
    dplyr::filter(transect=='oakMargin')  %>%
    dplyr::filter(time=='am') %>%
    dplyr::select(-transect, -position, -date, -time, -julian) %>%
    dplyr::group_by(week, positionX) %>%
    dplyr::summarize(crabSpiders=sum(Thomisidae..crab.spider.), .groups='keep')

  nonZeroPositions <- list()
  for (i in 1:length(weeks.list)) {
     zeroPositions <- zero.tibl %>% 
      filter(week==weeks.list[[i]] & crabSpiders == 0) %>% nrow()
    nonZeroPositions[[i]] <- 11 - zeroPositions
  }
  print(nonZeroPositions)
}
  
    zeroCounts <- tibble %>% filter(crabSpiders == 0) %>% nrow()

    if (zeroCounts > 9) {
      return(FALSE)
    }
    else {
      return(TRUE)
    }
}

assignClusters <- function(week, transect, time, tibble, debug) {
  
  # rationalize cluster assignments by week such that clusters
  # across multiple weeks can be visually compared
  
  # params :
  # week (discovered by utility function getWeeks()), 
  # transect (either 'control' or 'oakMargin')
  # time (either 'am' or 'pm')
  # tibl : this is the raw (all weeks, both clusters) tibble of all crab spiders
  # debug : (TRUE or FALSE for manual cluster list examination)
  
  # return: a list of 10 integers; values 1, 2, or 3 representing
  # updated cluster assignments
  
  # testing
  # > testParams(week=28, transect='control', tibl=bugs.tibl, debug=TRUE)
  # [1] "original clusters : 3 3 3 3 3 1 1 2 1 1"
  # [1] "revised clusters : 1 1 1 1 1 2 2 2 3 3"
  
  # based on examination of the spider counts for the season
  # I propose that there are 3 distinct clusters each week
  #
  # for positions 1-2-3 (expected typical cluster ID is '1')
  # and for positions 8-9-10 (expected typical cluster ID is '3')
  # kmeans() won't guarantee that they are structured this way,
  # (3s or 2s could be assigned at the beginning, and 1s or 2s at the end.)
  # this function tries to normalize these vectors so patterns 
  # between weeks can be compared.
  
  # kmeans() wants to evaluate groups of points specified in x,y coordinates
  # and assign them to clusters. 
  
  # the kmeans() return object encapsulates numerous params including 
  # 'cluster' (an arbitrary ID) ranked high-to-low or low-to-high.
  # so, to compare clusters across weeks, we need to re-assign cluster IDs
  # based on consistent ranks.
  
  # logic to enable cluster comparison across weeks:
  #
  # check cluster assignments for positions 1-2-3,  4-5-6, 8-9-10
  # I propose that positions 1-2-3 will likely be in the same cluster
  # as will positions 8-9-10. for a week's clusters assigned by kmeans() like                             
  # position              1 2 3 4 5 6 7 8 9 10
  # cluster assignment    3 1 1 x x x x 1 2 2
  
  # this function should notice that 3's should be 2's
  # and vice versa. It does not matter that the ranking criteria 'match' or 
  # 'align' from one week to another. Organizing ranks such that '1s' tend to be
  # at the beginning of the vineyard row and '3s' tend to be at the end of
  # the row will make a graphical comparison of clusters across weeks
  # easier 
  
  # for positions 1-2-3 (expected typical cluster ID is '1')
  # if "no matches" then no cluster assignment changes
  # if at least 2 matches of '1', then no cluster assignment changes
  # if at least 2 matches of '3', 
  #                             change every instance (for all positions) of '3' to '1', 
  #                             change every previous instance of '1' to '3'
  # if at least 2 matches of '2', 
  #                             change every instance (for all positions) of '2' to '1', 
  #                             change every previous instance of '1' to '2'
  # 
  # for positions 8-9-10 (expected typical cluster ID is '3')
  # if "no matches" then no cluster assignment changes
  # if at least 2 matches of '3', then no cluster assignment changes
  # if at least 2 matches of '2', 
  #                             change every instance (for all positions) of '2' to '3', 
  #                             change every previous instance of '3' to '2'
  # if at least 2 matches of '1', then no changes (this probably means that the clusters 
  # are totally scrambled)
  # 
  # note that this logic breaks if the true cluster for both groups 1-2-3 and 
  # 8-9-10 is '1' or '3' (very unlikely)
  # also, it does not correct 
  # position              1 2 3 4 5 6 7 8 9 10
  # cluster assignment    2 2 2 1 1 1 1 3 3 3
  # (this outcome is possible.....)

  # allow all data to be specified
  if (time == 'am' | time == 'pm') {
    bugsByWeek.tibl <- tibble %>%
      dplyr::filter(transect=={{transect}})  %>%
      dplyr::filter(time=={{time}}) %>%
      dplyr::select(-transect, -position, -date, -time, -julian) %>%
      dplyr::group_by(week, positionX) %>%
      dplyr::summarize(crabSpiders=sum(Thomisidae..crab.spider.), .groups='keep'  )
  }
  else {
    bugsByWeek.tibl <- tibble %>%
      dplyr::filter(transect=={{transect}})  %>%
      dplyr::select(-transect, -position, -date, -time, -julian) %>%
      dplyr::group_by(week, positionX) %>%
      dplyr::summarize(crabSpiders=sum(Thomisidae..crab.spider.), .groups='keep'  )
  }
  
  if (FALSE) {
  bugs.tibl %>%
    dplyr::filter(transect=='control')  %>%
    dplyr::filter(time=='pm') %>%
    dplyr::select(-transect, -position, -date, -time, -julian) %>%
    dplyr::group_by(week, positionX) %>%
    dplyr::summarize(crabSpiders=sum(Thomisidae..crab.spider.), .groups='keep')
  }

  # the result is, for each positionX in each week, crabSpider counts are   calculated
  #
  #  week   positionX   crabSpiders

  # isolate the week's data
  week.tibl <- bugsByWeek.tibl %>%
                  dplyr::filter(week=={{week}}) %>%
                  ungroup() %>% dplyr::select(-week)
  
  # verify that there is enough data to make 3 clusters
  if (enoughData(tibble=week.tibl, debug=debug) == TRUE) {
    
    # build the clusters
  
    # note: base::scale() is used to ensure that the original x (spider counts)
    # and y (trap position) axis scales do not skew the cluster assessment
    scaled.tibl <- as_tibble(scale(week.tibl))

    set.seed(20)

    clusters.kmeans <- scaled.tibl %>% 
      dplyr::select(positionX, crabSpiders) %>% 
      kmeans(centers = 3, iter.max = 500, nstart = 3)


    # get the cluster vector
    original.cluster.list <- clusters.kmeans$cluster

    # create lists capturing the original positions of 1, 2, and 3
    ones.list <- replace(original.cluster.list, original.cluster.list != 1, 0)
    twos.list <- replace(original.cluster.list, original.cluster.list != 2, 0)
    threes.list <- replace(original.cluster.list, original.cluster.list != 3, 0)
    # ones.list + twos.list + threes.list = original.scan.list
    orig.ones.list <- ones.list
    orig.twos.list <- twos.list
    orig.threes.list <- threes.list

    # are there at least 2 matches of '3' in position 1-2-3 ?
    if (sum(orig.threes.list[1:3]) > 3) {
      # there are multiple 3s in the first 3 positions, swap 3s to all 1s
      ones.list <- replace(orig.threes.list, orig.threes.list==3, 1)
      twos.list <- replace(orig.twos.list, orig.twos.list==2, 2)
      threes.list <- replace(orig.ones.list, orig.ones.list==1, 3)
    # are there at least 2 matches of '2' in position 1-2-3 ?  
    } else if (sum(orig.twos.list[1:3] == 2) > 2) {
      # there are multiple 2s in the first 3 positions, change all 2s to 1s
      ones.list <- replace(orig.twos.list, orig.twos.list==2, 1)
      twos.list <- replace(orig.ones.list, orig.ones.list==1, 2)
      threes.list <- replace(orig.threes.list, orig.threes.list==3, 3)
    } 

    revised.scan.list <- ones.list + twos.list + threes.list
    revised.ones.list <- ones.list
    revised.twos.list <- twos.list
    revised.threes.list <- threes.list

    if (debug==TRUE) {
      print("original")
      print(orig.ones.list)
      print(orig.twos.list)
      print(orig.threes.list)
      print(original.cluster.list)

      print("before 8:10")
      print(" ")
      print(revised.ones.list)
      print(revised.twos.list)
      print(revised.threes.list)
      print(revised.scan.list)
    }

  
    # are there at least 2 matches of '2' in position 8-9-10 ?
    if (sum(revised.twos.list[8:10]) > 2) {
      # there are multiple 2s in the last 3 positions, swap 2s to all 3s
      twos.list <- replace(revised.threes.list, revised.threes.list==3, 2)
      threes.list <- replace(revised.twos.list, revised.twos.list==2, 3)
      ones.list <- replace(revised.ones.list, revised.ones.list==1, 1)
    # are there at least 2 matches of '1' in position 8-9-10 ?  
    } else if (sum(revised.ones.list[8:10]) > 1) {
      # there are multiple 1s in the last 3 positions, change all 1s to 3s
      threes.list <- replace(revised.ones.list, revised.ones.list==1, 3)
      twos.list <- replace(revised.twos.list, revised.twos.list==2, 2)
      ones.list <- replace(revised.threes.list, revised.threes.list==3, 1)
    } 

    revised.scan.list <- ones.list + twos.list + threes.list
    revised.ones.list <- ones.list
    revised.twos.list <- twos.list
    revised.threes.list <- threes.list
  
    if (debug==TRUE) {
      print("after 8:10")
      print(" ")
      print(ones.list)
      print(twos.list)
      print(threes.list)
      print(revised.scan.list)
    }
  
    # two final rules
    # position 1 should always be assigned to cluster 1
    # position 10 should always be assigned to cluster 3
    # handling only the situation where a 2 exists in either position
    # (not both)
  
    # is there a '2' in position 1 ?
    if (revised.scan.list[1] == 2) {
      # change all 2s to 1s and 1s to 2s
      ones.list <- replace(revised.twos.list, revised.twos.list==2, 1)
      twos.list <- replace(revised.ones.list, revised.ones.list==1, 2) 
      threes.list <- revised.threes.list
    } 
    # is there a '2' in position 10 ? 
    if (revised.scan.list[10] == 2) {
     # change all 2s to 3s and 3s to 2s
      threes.list <- replace(revised.twos.list, revised.twos.list==2, 3)
      twos.list <- replace(revised.threes.list, revised.threes.list==3, 2) 
      ones.list <- revised.ones.list
    } 

    revised.scan.list <- ones.list + twos.list + threes.list
    
  }
  else {
    # enoughData() returned FALSE, not enough data to build clusters
    # set the revised scan list to zeros (no clusters assigned)
    original.cluster.list <- c(0,0,0,0,0,0,0,0,0,0)
    revised.scan.list <- c(0,0,0,0,0,0,0,0,0,0)
  }
  
  if (debug==TRUE) {
      print(paste("week : ", week, sep=''))
      printString <- paste("original clusters : ",
                         do.call("paste",
                                as.list(original.cluster.list)), sep='')
      print(printString)
      printString <- paste("revised clusters : ", 
                         do.call("paste", 
                                 as.list(revised.scan.list)), sep='')
      print(printString)
    }
  

  # return the cluster assignments as a single column tibble
  #
  revised.scan.list.tibl <- as_tibble(revised.scan.list)
  # default column name is 'value'
  # this is a dplyr hack to specify the column name
  # https://stackoverflow.com/questions/45472480/how-to-rename-a-column-to-a-variable-name-in-a-tidyverse-way
  colName <- as.name(paste("week", week, sep=''))
  revised.scan.list.tibl <- revised.scan.list.tibl %>%
                              dplyr::rename(!!colName := value)
  
  return(revised.scan.list.tibl)

}
	
```

